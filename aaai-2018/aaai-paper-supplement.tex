\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:

\pdfinfo{
  /Title (Supplement for ``Algorithms for Estimating Trends in Global Temperature
  Volatility'' )
   /Author (Khodadadi and McDonald)}

\setcounter{secnumdepth}{2}  


% \usepackage[bookmarks=false]{hyperref}       
% \AtBeginDocument{\NoHyper} % turn off all links, colors
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% For citations
%\usepackage[sort,numbers,square]{natbib}

\newcommand{\citet}[1]
{\citeauthor{#1} ̃\shortcite{#1}}
\newcommand{\citep}{\cite}
\newcommand{\citealp}[1]
{\citeauthor{#1} ̃\citeyear{#1}}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
%\hypersetup{backref,colorlinks=true,citecolor=blue,linkcolor=blue,urlcolor=blue}

%\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage{subfigure} 
\graphicspath{{../Figures/}}


%Arash
\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{siunitx}
\usepackage{comment}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}

\usepackage{cleveref}

\newtheorem{theorem}{Theorem}%[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\autoref}{\Cref}

\usepackage{gensymb}
\usepackage{xcolor}
\newcommand{\attn}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\one}{\mathbf{1}}
\renewcommand{\algorithmiccomment}[1]{\hfill $\rhd$ #1}
\newcommand{\given}{\;\vert\;}
\newcommand*{\algorithmautorefname}{Algorithm}%
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\y}{\textrm{year}}
\DeclareMathOperator*{\prox}{\bf prox}
\DeclareMathOperator*{\sign}{sign}

\usepackage{xr}
\externaldocument{khodadadi-mcdonald-aaai19-camera}

\begin{document} 

\title{Supplement for ``Algorithms for Estimating Trends in Global Temperature Volatility''}

\author{Arash Khodadadi \and Daniel J. McDonald\\
 Department of Statistics\\
 Indiana University\\
 Bloomington, IN 47408 \\
 \{arakhoda,dajmcdon\}@indiana.edu}

% \author{Arash Khodadadi \and Daniel J. McDonald\\
%  Department of Statistics\\
%  Indiana University\\
%  Bloomington, IN 47408 \\
%  \{arakhoda,dajmcdon\}@indiana.edu}





\maketitle


\appendix
\section{PDIP for $\ell_1$ Trend Filtering of variance}
\label{sec:app_l1tf_var}

In this appendix we provide more details on how to solve the optimization problem with the objective specified in \autoref{eq:l1tf_var} using PDIP. The objective function is convex but not differentiable. Therefore, to be able to use PDIP we first need to derive the dual of this problem. We note that this is a generalized LASSO problem \citep{TibshiraniTaylor2011}. The dual of a generalized LASSO with the objective $f(x)+\lambda \norm{ Dx }_1$ is:  
\begin{align}
\min_\nu&\quad f^*(-D^\top\nu) & \mbox{s.t.}&\quad \norm{ \nu }_\infty \le \lambda
\end{align}
where $f^*(\cdot)$ is the Fenchel conjugate of $f$: $f^*(u)=\max_x u^\top x-f(x)$. It is simple to show that for the objective function of Equation \ref{eq:l1tf_var}
\begin{equation}
f^*(u)=\sum_t (u_t-1)\log\frac{y_t^2}{1-u_t} + u_t-1.
\label{eq:conj}
\end{equation}

Each iteration of PDIP involves computing a search direction by taking a Newton step for the system of nonlinear equations $r_w(v,\mu_1,\mu_2)=0$, where $w>0$ is a parameter and
\begin{equation}
\begin{aligned}
  &r_w(v,\mu_1,\mu_2)
  :=
	\begin{bmatrix}
	r_{dual}\\
	r_{cent}	
	\end{bmatrix}\\ &=
  \begin{bmatrix}
    \nabla f^*(-D^\top v) + \mu_1 - \mu_2\\
    -\mu_1(v-\lambda\one)+\mu_2(v + \lambda\one) -w^{-1}\one
  \end{bmatrix}
\end{aligned}
\label{eq:resid}
\end{equation}
where $\mu_1$ and $\mu_2$ are dual variables for the $\ell_\infty$ constraint. Let $A=[\nabla r_{dual}^\top , \nabla r_{cent}^\top]^\top$. The newton step takes the following form
\begin{equation}
r_w(v,\mu_1,\mu_2)+A
\begin{bmatrix}
	\nabla v\\
	\nabla \mu_1\\
	\nabla \mu_2	
	\end{bmatrix}= 0
\label{eq:newton_step}
\end{equation}
with
\begin{equation}
A=
\begin{bmatrix}
	\nabla^2 f^*(-D^\top v) & I & -I\\
	-\mathbf{diag(\mu_1)}\one & -v+\lambda\one & \mathbf{0}\\
	\mathbf{diag(\mu_2)}\one & v+\lambda\one & \mathbf{0}
	\end{bmatrix}.
\label{eq:delta_r}
\end{equation}
Therefore, to perform the Newton step we need to compute $\nabla
f^*(-D^\top v)$ and $\nabla^2 f^*(-D^\top v)$. 
It is straightforward to show that
\begin{align}
\nabla f^*(-D^\top v) &=  -\nabla_u f^*(u) D^\top ,\\
u&=-D^\top v, \\
 (\nabla_u f^*(u))_j&=\log\bigg(\frac{y_j^2}{1-u_j}\bigg), \\
\nabla^2 f^*(-D^\top v)&=D\nabla_u^2 f^*(u)D^\top,\\  
(\nabla_u^2 f^*(u))_j&=\mathbf{diag}\bigg(\frac{1}{1-u_j}\bigg).
\end{align}


Having computed the conjugate function and its gradient and Jacobian,
now we can use a number of convex optimization software packages which
have an implementation of PDIP to solve the optimization problem with
the objective function \autoref{eq:l1tf_var}. We chose the python API of
the \texttt{cvxopt} package~\citep{andersen_cvxopt:_2013}. 




\section{PDIP Update in~\autoref{alg:conADMM}}
\label{sec:app_consADMM}

In this section we give more details on performing the $x$-update step in Algorithm 1. We need to solve the following optimization problem:
\begin{align}
\hat{x} &:=\argmin_{x} \bigg( \sum_{j=1}^{n_b} (x_j +
                y_j^2e^{-x_j})\\
  &\quad+ (\rho/2) \lVert x-\tilde{z} + u \lVert_2^2 + \Lambda^\top |D x| \bigg)
\label{eq:x_update_opt}
\end{align}
where $n_b$ is the number of local variables in each sub-cube in \autoref{fig:data_cube}, and for ease of notation we have dropped the subscript $i$ and superscript $m$. 

The matrix $D$ has the following form:  $D=[D_{temp}|D_{spat}]$. The matrix $D_{temp}$ is the following block-diagonal matrix and corresponds to the temporal penalty: 
\begin{equation}
D_{temp}=\begin{bmatrix}
D_t &  & \\ 
& \ddots & \\
&  & D_t
\end{bmatrix},
\label{eq:d_t_matrix}
\end{equation}
where $D_t$ was first introduced in Section 2 of the main text and has the following form:
\begin{equation}
 D_t=\begin{bmatrix}
 1 & -2 & 1 &  &  &  &\\ 
 & 1 & -2 & 1 &  &0  &\\ 
 &  &  & \ddots &  &  &\\ 
 & 0 & & 1 & -2 & 1 &  \\ 
 &  &  &   & 1 & -2 & 1 
 \end{bmatrix}.
\label{eq:d_matrix}
\end{equation}

The number of the diagonal blocks in $D_{temp}$ is equal to the grid
size $n_r \times n_c$. Each row of the matrix $D_{spat}$ corresponds
to one spatial constraint in Equation \eqref{eq:l1tf_var_st} in the
text. For example, the first $T$ rows correspond to
$|h_{11t}-h_{21t}|$ for $t=1,...,T$, the next $T$ rows correspond to
$|h_{11t}-h_{12t}|$, and so on.  


This optimization problem, is again a generalized LASSO problem with
$f(x)=\sum_{j=1}^{n_b} (x_j + y_j^2e^{-x_j}) + (\rho/2) \lVert
x-\tilde{z} + u \lVert_2^2$.  

As it was explained in Appendix A, the dual of this optimization
problem is: $\min_\nu f^*(-D^\top\nu)$ with the constraints $|\nu_k|
\le \Lambda_k$. To use PDIP we first need to compute the conjugate
function $f^*(\cdot)$. We have: 
\begin{equation}
\begin{aligned}
&f^*(\xi)  = \max_x  \xi^\top x - f(x)\\
& =  \max_x \sum_{j=1}^{n_b} (\xi_jx_j - x_j - y_j^2e^{-x_j} -
(\rho/2)(x_j-\tilde{z}_j+u_j)). 
\end{aligned}
\label{eq:conjugate}
\end{equation}
Setting the derivative of the terms inside the summation to 0, we obtain:
\begin{equation}
\xi_j-y_j^2e^{-x_j^*}-\rho x_j^* + \rho (\tilde{z}_j-u_j)=0,
\label{eq:x_start}
\end{equation}
where $x^*$ is the maximizer in \ref{eq:conjugate}. Then, it
can be shown that $x_j^*$ which satisfies \eqref{eq:x_start} can be
obtained as follows: 
\begin{align}
x^*_j & = \mathscr{W}\bigg(\frac{y_j^2}{\rho} e^{\phi_j} \bigg) - \phi_j, \\
\phi_j & =\frac{1-\xi_j-\rho(\tilde{z}_j-u_j)}{\rho}.
\end{align}
In this equation, $\mathscr{W}(\cdot)$ is the Lambert W function \cite{corless_lambertw_1996}. Finally, the conjugate function is: $f^*(\xi) = \sum_{j=1}^{n_b} (\xi_jx^*_j - x^*_j - y_j^2e^{-x^*_j} - (\rho/2)(x^*_j-\tilde{z}_j+u_j))$.

To use PDIP, we also need to evaluate $\nabla f^*$ and $\nabla^2 f^*$. First note that $\frac{\partial \mathscr{W}(q)}{\partial q} = \frac{\mathscr{W}(q)}{q(1+\mathscr{W}(q))}$ and $\frac{\partial^2 \mathscr{W}(q)}{\partial q^2} = - \frac{\mathscr{W}^2(q)(\mathscr{W}(q)+q)}{q^2(1+\mathscr{W}(q))^3}$. Using the chain rule we get:
%\x^*_j + \xi_j + \frac{\partial \x^*_j}{\partial \xi_j}
\begin{equation}
\frac{\partial f^*(\xi)}{\partial \xi_j}  =  x^*_j  + \frac{\partial
  x^*_j}{\partial \xi_j} \left[ \xi_j -1 + y_j^2 e^{-x_j^*} + \rho
  (\tilde{z}_j - u_j - x_j^*) \right], 
\label{eq:d_f*_start}
\end{equation}
where we have
\begin{equation}
\frac{\partial x_j^*}{\partial \xi_j}  = \frac{1}{\rho(1+\mathscr{W}((y_j^2/\rho) e^{-\phi_j} ))}.
\label{eq:d_x*_start}
\end{equation}

By some tedious but straightforward computation we can obtain the second derivatives:
\begin{align}
\frac{\partial^2 f^*(\xi)}{\partial \xi_j^2} 
  & =  \frac{\partial
    x_j^*}{\partial \xi_j}
    - \rho \frac{\partial^2
    x_j^*}{\partial
    \xi_j^2} \bigg[ \phi_j
    +x_j^* - \tilde{z}_j +
    u_j \bigg],\\ 
  & \quad + \frac{\partial x_j^*}{\partial \xi_j} \bigg[ 1-y_j^2
    \frac{\partial x_j^*}{\partial \xi_j} e^{-x_j^*} -\rho
    \frac{\partial x_j^*}{\partial \xi_j} \bigg], \label{eq:d2_f*_start}\\
  \frac{\partial^2 x_j^*}{\partial \xi_j^2}  &=
                                               \frac{\mathscr{W}((y_j^2/\rho)
                                               e^{-\phi_j}
                                               )}{\rho^2(1+\mathscr{W}((y_j^2/\rho)
                                               e^{-\phi_j} ))^3} .
\label{eq:d2_x*_start}
\end{align}

\section{Proof of Lemma 1}
\label{sec:proof-lemma-1}

\begin{proof}
  If $f(x)=\sum_k f_k(x_k)$ then $[\prox_{\mu f}(x)]_k =
  \prox_{\mu f_k}(u_k)$. So 
  $[\prox_{\mu f}(u)]_k=\min_{x_k} \,\,
  \mu\big(x_k+y_{k}^2e^{-x_{k}}\big)+\frac{1}{2}  (x_k-u_k)^2.$
  Setting the derivative to 0 and solving for $u_k$ gives the
  result. Similarly, $[\prox_{\rho g}(u)]_\ell=\rho
  \lambda_\ell |z_\ell|+1/2(z_\ell-u_\ell)^2$. This is not differentiable,
  but the solution must satisfy $\rho \cdot \lambda_\ell \cdot \partial
  \big(|z_\ell| \big)=u_\ell-z_\ell$ where $\partial \big(|z_\ell| \big)$ is the
  sub-differential of $|z_\ell|$. The solution is the soft-thresholding
  operator $S_{\rho\lambda_\ell}(u_\ell)$.
\end{proof}


 
\bibliographystyle{aaai}
\bibliography{aaai-references}


\end{document}